{"cells":[{"cell_type":"markdown","metadata":{},"source":["# global variables"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:48:58.792018Z","iopub.status.busy":"2023-08-31T07:48:58.791441Z","iopub.status.idle":"2023-08-31T07:48:58.799896Z","shell.execute_reply":"2023-08-31T07:48:58.798807Z","shell.execute_reply.started":"2023-08-31T07:48:58.791971Z"},"trusted":true},"outputs":[],"source":["# check environment\n","from os import environ\n","IS_KAGGLE_ENVIRONMENT = 'KAGGLE_KERNEL_RUN_TYPE' in environ\n","\n","# models path\n","MODELS_PATH = '/kaggle/working/models/' if IS_KAGGLE_ENVIRONMENT else 'data/models/'\n","MODEL_NAME = 'mobilenetv2-deeplabv3plus-ssdlite'\n","\n","# data options\n","INPUT_IMAGE_SHAPE = (480, 640, 3)\n","LABELS_CODES = [0, 1, 2, 3]\n","LABEL_CODE_BACKGROUND = 0\n","NUMBER_OF_CLASSES = len(LABELS_CODES)\n","\n","# object detection options\n","STANDARD_DEVIATIONS_CENTROIDS_OFFSETS = (0.1, 0.1, 0.2, 0.2)\n","\n","# labels conversions\n","LABEL_CODE_TO_DESC = {\n","    1: 'monorail',\n","    2: 'person',\n","    3: 'forklift'\n","}\n","LABEL_CODE_TO_COLOR = {\n","    1: 'red',\n","    2: 'green',\n","    3: 'blue'\n","}\n","\n","# tensorflow options\n","BATCH_SIZE = 16\n","SEED = 1993"]},{"cell_type":"markdown","metadata":{},"source":["# kaggle setup"]},{"cell_type":"markdown","metadata":{},"source":["## clone repository and setup"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:48:58.802594Z","iopub.status.busy":"2023-08-31T07:48:58.802142Z","iopub.status.idle":"2023-08-31T07:49:04.462098Z","shell.execute_reply":"2023-08-31T07:49:04.460865Z","shell.execute_reply.started":"2023-08-31T07:48:58.802555Z"},"trusted":true},"outputs":[],"source":["if IS_KAGGLE_ENVIRONMENT:\n","    # check if repo folder exists, eventually delete it\n","    %cd '/kaggle/working/'\n","    import os\n","    if os.path.exists('ssd-segmentation'):\n","        !rm -r 'ssd-segmentation'\n","\n","    # clone github repository\n","    !git clone 'https://github.com/matteo-stat/ssd-segmentation.git'\n","\n","    # change working directory to cloned repository folder\n","    %cd '/kaggle/working/ssd-segmentation'\n","\n","    # change branch\n","    # !git checkout 'main'\n","\n","    # show working directory content\n","    !ls"]},{"cell_type":"markdown","metadata":{},"source":["# dependecies"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:49:04.464032Z","iopub.status.busy":"2023-08-31T07:49:04.463661Z","iopub.status.idle":"2023-08-31T07:49:13.705438Z","shell.execute_reply":"2023-08-31T07:49:13.704005Z","shell.execute_reply.started":"2023-08-31T07:49:04.463997Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","tf.keras.saving.get_custom_objects().clear()\n","tf.random.set_seed(SEED)\n","\n","import random\n","random.seed(SEED)\n","\n","import json\n","import csv\n","import numpy as np\n","from matplotlib import pyplot as plt, patches\n","from PIL import Image\n","import ssdseglib"]},{"cell_type":"markdown","metadata":{},"source":["# default bounding boxes"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:49:13.708698Z","iopub.status.busy":"2023-08-31T07:49:13.708020Z","iopub.status.idle":"2023-08-31T07:49:13.720230Z","shell.execute_reply":"2023-08-31T07:49:13.719030Z","shell.execute_reply.started":"2023-08-31T07:49:13.708661Z"},"trusted":true},"outputs":[],"source":["# create default bounding boxes\n","boxes_default = ssdseglib.boxes.DefaultBoundingBoxes(\n","    feature_maps_shapes=((30, 40), (15, 20), (13, 18), (7, 9), (4, 5)),\n","    feature_maps_aspect_ratios=(1, 2, 3, 4, 5, 6, 1/2, 1/3, 1/4),\n","    centers_padding_from_borders_percentage=(0.025, 0.05, 0.1, 0.125, 0.2),    \n","    boxes_scales=(0.18, 0.95),\n","    additional_square_box=True,\n",")\n","\n","# rescale default bounding boxes to input image shape\n","boxes_default.rescale_boxes_coordinates(image_shape=INPUT_IMAGE_SHAPE[:2])"]},{"cell_type":"markdown","metadata":{},"source":["# data encoder"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:49:13.721535Z","iopub.status.busy":"2023-08-31T07:49:13.721217Z","iopub.status.idle":"2023-08-31T07:49:13.840683Z","shell.execute_reply":"2023-08-31T07:49:13.839314Z","shell.execute_reply.started":"2023-08-31T07:49:13.721509Z"},"trusted":true},"outputs":[],"source":["# create a data reader encoder\n","data_reader_encoder = ssdseglib.datacoder.DataEncoderDecoder(\n","    num_classes=NUMBER_OF_CLASSES,\n","    image_shape=INPUT_IMAGE_SHAPE[:2],\n","    xmin_boxes_default=boxes_default.get_boxes_coordinates_xmin(coordinates_style='ssd'),\n","    ymin_boxes_default=boxes_default.get_boxes_coordinates_ymin(coordinates_style='ssd'),\n","    xmax_boxes_default=boxes_default.get_boxes_coordinates_xmax(coordinates_style='ssd'),\n","    ymax_boxes_default=boxes_default.get_boxes_coordinates_ymax(coordinates_style='ssd'),\n","    iou_threshold=0.5,\n","    standard_deviations_centroids_offsets=STANDARD_DEVIATIONS_CENTROIDS_OFFSETS,\n","    augmentation_horizontal_flip=True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# input data"]},{"cell_type":"markdown","metadata":{},"source":["## load metadata"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:49:13.842690Z","iopub.status.busy":"2023-08-31T07:49:13.842238Z","iopub.status.idle":"2023-08-31T07:49:13.867224Z","shell.execute_reply":"2023-08-31T07:49:13.866155Z","shell.execute_reply.started":"2023-08-31T07:49:13.842646Z"},"trusted":true},"outputs":[],"source":["# training\n","data = []\n","\n","# train\n","with open('data/train.json', 'r') as f:\n","    data.extend(json.load(f))\n","\n","# train additional - persons\n","with open('data/train-additional-persons.json', 'r') as f:\n","    data.extend(json.load(f))\n","\n","# train additional - forklifts\n","with open('data/train-additional-forklifts.json', 'r') as f:\n","    data.extend(json.load(f))\n","\n","# the training set it's small and the validation set even smaller..\n","# it's so small that probably any metrics on it won't be particularly reliable \n","# at this point maybe it's just better to use the validation set as additional training data\n","with open('data/eval-persons-forklifts.json', 'r') as f:\n","    data.extend(json.load(f))\n","\n","# unpack train metadata into separate lists\n","path_files_images_train, path_files_masks_train, path_files_labels_boxes_train = map(list, zip(*data))\n","\n","# test\n","with open('data/test.json', 'r') as f:\n","    path_files_images_test, path_files_masks_test, path_files_labels_boxes_test = map(list, zip(*json.load(f)))\n","\n","# replace local data directory with kaggle input directory\n","if IS_KAGGLE_ENVIRONMENT:\n","    path_data_kaggle = '/kaggle/input/ssd-segmentation-dataset/'\n","    path_files_images_train = [path.replace('data/', f'{path_data_kaggle}data/') for path in path_files_images_train]\n","    path_files_masks_train = [path.replace('data/',  f'{path_data_kaggle}data/') for path in path_files_masks_train]\n","    path_files_labels_boxes_train = [path.replace('data/',  f'{path_data_kaggle}data/') for path in path_files_labels_boxes_train]\n","\n","    path_files_images_test = [path.replace('data/',  f'{path_data_kaggle}data/') for path in path_files_images_test]\n","    path_files_masks_test = [path.replace('data/',  f'{path_data_kaggle}data/') for path in path_files_masks_test]\n","    path_files_labels_boxes_test = [path.replace('data/',  f'{path_data_kaggle}data/') for path in path_files_labels_boxes_test]"]},{"cell_type":"markdown","metadata":{},"source":["## tensorflow datasets"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:49:13.868958Z","iopub.status.busy":"2023-08-31T07:49:13.868543Z","iopub.status.idle":"2023-08-31T07:49:15.230354Z","shell.execute_reply":"2023-08-31T07:49:15.229128Z","shell.execute_reply.started":"2023-08-31T07:49:13.868901Z"},"trusted":true},"outputs":[],"source":["# training\n","ds_train = (\n","    tf.data.Dataset.from_tensor_slices((path_files_images_train, path_files_masks_train, path_files_labels_boxes_train))\n","    .shuffle(buffer_size=len(path_files_images_train))\n","    .map(data_reader_encoder.read_and_encode, num_parallel_calls=tf.data.AUTOTUNE)\n","    .batch(batch_size=BATCH_SIZE)\n","    .map(ssdseglib.datacoder.augmentation_rgb_channels, num_parallel_calls=tf.data.AUTOTUNE)\n","    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",")\n","\n","# test\n","ds_test = (\n","    tf.data.Dataset.from_tensor_slices(path_files_images_test)\n","    .map(ssdseglib.datacoder.read_image, num_parallel_calls=tf.data.AUTOTUNE)\n","    .batch(batch_size=BATCH_SIZE)\n","    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# weighted losses for model training"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:49:15.231986Z","iopub.status.busy":"2023-08-31T07:49:15.231621Z","iopub.status.idle":"2023-08-31T07:49:15.239311Z","shell.execute_reply":"2023-08-31T07:49:15.238318Z","shell.execute_reply.started":"2023-08-31T07:49:15.231954Z"},"trusted":true},"outputs":[],"source":["# weighted loss for semantic segmentation\n","dice_loss = ssdseglib.losses.dice(classes_weights=(1.0, 1.0, 1.0, 1.0))"]},{"cell_type":"markdown","metadata":{},"source":["# weighted metrics for model training"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:49:15.241210Z","iopub.status.busy":"2023-08-31T07:49:15.240800Z","iopub.status.idle":"2023-08-31T07:49:15.251802Z","shell.execute_reply":"2023-08-31T07:49:15.250650Z","shell.execute_reply.started":"2023-08-31T07:49:15.241182Z"},"trusted":true},"outputs":[],"source":["# weighted metrics for semantic segmentation\n","jaccard_iou_segmentation_masks_metric = ssdseglib.metrics.jaccard_iou_segmentation_masks(classes_weights=(0., 1/3, 1/3, 1/3))\n","\n","# weighted metrics for boxes classification\n","categorical_accuracy_metric = ssdseglib.metrics.categorical_accuracy(classes_weights=(0., 1/3, 1/3, 1/3))\n","\n","# metrics for boxes regression\n","jaccard_iou_bounding_boxes_metric = ssdseglib.metrics.jaccard_iou_bounding_boxes(\n","    center_x_boxes_default=data_reader_encoder.center_x_boxes_default,\n","    center_y_boxes_default=data_reader_encoder.center_y_boxes_default,\n","    width_boxes_default=data_reader_encoder.width_boxes_default,\n","    height_boxes_default=data_reader_encoder.height_boxes_default,\n","    standard_deviations_centroids_offsets=STANDARD_DEVIATIONS_CENTROIDS_OFFSETS\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# model"]},{"cell_type":"markdown","metadata":{},"source":["## architecture"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:49:15.255733Z","iopub.status.busy":"2023-08-31T07:49:15.254887Z","iopub.status.idle":"2023-08-31T07:49:15.268734Z","shell.execute_reply":"2023-08-31T07:49:15.267553Z","shell.execute_reply.started":"2023-08-31T07:49:15.255698Z"},"trusted":true},"outputs":[],"source":["# model builder\n","model_builder = ssdseglib.models.MobileNetV2SsdSegBuilder(\n","    input_image_shape=INPUT_IMAGE_SHAPE,\n","    number_of_boxes_per_point=[\n","        len(aspect_ratios) + (1 if boxes_default.additional_square_box else 0)\n","        for aspect_ratios in boxes_default.feature_maps_aspect_ratios\n","    ],\n","    number_of_classes=NUMBER_OF_CLASSES,\n","    center_x_boxes_default=boxes_default.get_boxes_coordinates_center_x(coordinates_style='ssd'),\n","    center_y_boxes_default=boxes_default.get_boxes_coordinates_center_y(coordinates_style='ssd'),\n","    width_boxes_default=boxes_default.get_boxes_coordinates_width(coordinates_style='ssd'),\n","    height_boxes_default=boxes_default.get_boxes_coordinates_height(coordinates_style='ssd'),\n","    standard_deviations_centroids_offsets=STANDARD_DEVIATIONS_CENTROIDS_OFFSETS\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:49:15.270523Z","iopub.status.busy":"2023-08-31T07:49:15.270195Z","iopub.status.idle":"2023-08-31T07:49:17.453867Z","shell.execute_reply":"2023-08-31T07:49:17.452696Z","shell.execute_reply.started":"2023-08-31T07:49:15.270494Z"},"trusted":true},"outputs":[],"source":["# model for training\n","model = model_builder.get_model_for_training(\n","    segmentation_architecture='deeplabv3plus',\n","    object_detection_architecture='ssdlite',\n","    segmentation_dilation_rates=(2, 4, 8)\n",")\n","\n","# or maybe load a trained model and continue the training\n","# model = tf.keras.models.load_model('/kaggle/working/mobilenetv2-ssdseg.keras', compile=False)\n","\n","# print model summary\n","# model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## optimizer"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:49:17.456025Z","iopub.status.busy":"2023-08-31T07:49:17.455506Z","iopub.status.idle":"2023-08-31T07:49:17.496958Z","shell.execute_reply":"2023-08-31T07:49:17.495724Z","shell.execute_reply.started":"2023-08-31T07:49:17.455979Z"},"trusted":true},"outputs":[],"source":["# learning rate scheduler\n","learning_rate_scheduler = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n","    boundaries=[40, 70, 100, 120],\n","    values=[0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",")\n","\n","# optimizer\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_scheduler)"]},{"cell_type":"markdown","metadata":{},"source":["## compile"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:49:17.498705Z","iopub.status.busy":"2023-08-31T07:49:17.498371Z","iopub.status.idle":"2023-08-31T07:49:17.528859Z","shell.execute_reply":"2023-08-31T07:49:17.527557Z","shell.execute_reply.started":"2023-08-31T07:49:17.498675Z"},"trusted":true},"outputs":[],"source":["# each ouput has its own loss and metrics\n","model.compile(\n","    optimizer=optimizer,\n","    loss={\n","        'output-mask': dice_loss,\n","        'output-labels': ssdseglib.losses.confidence_loss,\n","        'output-boxes': ssdseglib.losses.localization_loss\n","    },\n","    loss_weights={\n","        'output-mask': 1.0,\n","        'output-labels': 1.0,\n","        'output-boxes': 1.0\n","    },\n","    metrics={\n","        'output-mask': jaccard_iou_segmentation_masks_metric,\n","        'output-labels': categorical_accuracy_metric,\n","        'output-boxes': jaccard_iou_bounding_boxes_metric,\n","    }\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## early stopping"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:49:17.530678Z","iopub.status.busy":"2023-08-31T07:49:17.530286Z","iopub.status.idle":"2023-08-31T07:49:17.537064Z","shell.execute_reply":"2023-08-31T07:49:17.535766Z","shell.execute_reply.started":"2023-08-31T07:49:17.530643Z"},"trusted":true},"outputs":[],"source":["early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor='loss',\n","    min_delta=0.1,\n","    patience=25,\n","    verbose=1,\n","    restore_best_weights=True,\n","    start_from_epoch=0,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## training model"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T07:49:17.538748Z","iopub.status.busy":"2023-08-31T07:49:17.538346Z","iopub.status.idle":"2023-08-31T07:49:46.465963Z","shell.execute_reply":"2023-08-31T07:49:46.464198Z","shell.execute_reply.started":"2023-08-31T07:49:17.538711Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/140\n"]},{"ename":"NotFoundError","evalue":"Graph execution error:\n\n/kaggle/input/ssd-segmentation-data/data/train/2370_mask.png; No such file or directory\n\t [[{{node ReadFile_1}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_33666]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# fit the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m140\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\n/kaggle/input/ssd-segmentation-data/data/train/2370_mask.png; No such file or directory\n\t [[{{node ReadFile_1}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_33666]"]}],"source":["# fit the model\n","history = model.fit(\n","    ds_train,\n","    epochs=140,\n","    callbacks=[early_stopping]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### training history"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-08-30T18:56:31.344353Z","iopub.status.idle":"2023-08-30T18:56:31.346863Z","shell.execute_reply":"2023-08-30T18:56:31.346596Z","shell.execute_reply.started":"2023-08-30T18:56:31.346567Z"},"trusted":true},"outputs":[],"source":["# plot training loss and validation loss\n","plt.figure(figsize=(10, 6))\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## save weights"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-08-30T18:56:31.349913Z","iopub.status.idle":"2023-08-30T18:56:31.352575Z","shell.execute_reply":"2023-08-30T18:56:31.352357Z","shell.execute_reply.started":"2023-08-30T18:56:31.352333Z"},"trusted":true},"outputs":[],"source":["# save model\n","model.save(f'{MODELS_PATH}{MODEL_NAME}.keras')"]},{"cell_type":"markdown","metadata":{},"source":["## inference model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-26T22:09:44.935753Z","iopub.status.busy":"2023-08-26T22:09:44.935319Z","iopub.status.idle":"2023-08-26T22:09:48.201728Z","shell.execute_reply":"2023-08-26T22:09:48.200781Z","shell.execute_reply.started":"2023-08-26T22:09:44.935719Z"},"trusted":true},"outputs":[],"source":["# load trained model\n","model_trained = tf.keras.models.load_model(f'{MODELS_PATH}{MODEL_NAME}.keras', compile=False)\n","\n","# transfer weights\n","model_inference = model_builder.get_model_for_inference(\n","    model_trained=model_trained,\n","    max_number_of_boxes_per_class=15,\n","    max_number_of_boxes_per_sample=25,\n","    boxes_iou_threshold=0.5,\n","    labels_probability_threshold=0.6,\n","    suppress_background_boxes=False\n",")\n","\n","# print model summary\n","# model_inference.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## evaluation"]},{"cell_type":"markdown","metadata":{},"source":["### evaluation dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# get predictions for the whole test set\n","segmentation_pred_batch, detection_pred_batch = model_inference.predict(ds_test)\n","\n","# split and format predictions as required by the evaluators\n","segmentation_pred_batch = segmentation_pred_batch.astype(np.float32)\n","labels_pred_batch = detection_pred_batch[:, :, 0].astype(np.int32)\n","confidences_pred_batch = detection_pred_batch[:, :, 1].astype(np.float32)\n","boxes_pred_batch = detection_pred_batch[:, :, 2:].astype(np.float32)"]},{"cell_type":"markdown","metadata":{},"source":["### jaccard iou"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# evaluate iou for each class\n","iou_per_class = ssdseglib.evaluators.jaccard_iou_semantic_segmentation(\n","    masks_pred_batch=segmentation_pred_batch,\n","    path_files_masks=path_files_masks_test,\n","    labels_codes=LABELS_CODES,\n","    label_code_background=LABEL_CODE_BACKGROUND    \n",")\n","\n","# calculate the maximum label length\n","length_longest_label = max(len(label) for label in LABEL_CODE_TO_DESC.values())\n","\n","# print\n","print('\\n****************')\n","print(f'***   IoU    ***')\n","print('****************')\n","for label, iou in iou_per_class.items():\n","    print(f'> {LABEL_CODE_TO_DESC[label]:>{length_longest_label}}: {iou:2.2f}')\n","print('----------------')\n","print(f'> {\"mIoU@\":>{length_longest_label}}: {sum(iou_per_class.values()) / len(iou_per_class):.2f}') "]},{"cell_type":"markdown","metadata":{},"source":["### average precision"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# set the iou thresholds to use for evaluate average precision in object detection\n","iou_thresholds_object_detection = [0.5, 0.75]\n","\n","# calculate the maximum label length\n","length_longest_label = max(len(label) for label in LABEL_CODE_TO_DESC.values())\n","\n","# for each iou threshold calculate AP and mAP\n","for iou_threshold in iou_thresholds_object_detection:\n","    average_precision_per_class = ssdseglib.evaluators.average_precision_object_detection(\n","        labels_pred_batch=labels_pred_batch,\n","        confidences_pred_batch=confidences_pred_batch,\n","        boxes_pred_batch=boxes_pred_batch,\n","        iou_threshold=iou_threshold,\n","        path_files_labels_boxes=path_files_labels_boxes_test,\n","        labels_codes=LABELS_CODES,\n","        label_code_background=LABEL_CODE_BACKGROUND\n","    )\n","\n","    # iou threshold formatted for printing\n","    iou_threshold = format(iou_threshold, '.2f').lstrip('0')\n","\n","    # print\n","    print('\\n****************')\n","    print(f'***  AP@{iou_threshold}  ***')\n","    print('****************')\n","    for label, average_precision in average_precision_per_class.items():\n","        print(f'> {LABEL_CODE_TO_DESC[label]:>{length_longest_label}}: {average_precision:2.2f}')\n","    print('----------------')\n","    print(f'> {f\"mAP@{iou_threshold}\":>{length_longest_label}}: {sum(average_precision_per_class.values()) / len(average_precision_per_class):.2f}')    "]},{"cell_type":"markdown","metadata":{},"source":["# predict"]},{"cell_type":"markdown","metadata":{},"source":["## plot some predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-26T22:13:26.328027Z","iopub.status.busy":"2023-08-26T22:13:26.327586Z","iopub.status.idle":"2023-08-26T22:13:38.714577Z","shell.execute_reply":"2023-08-26T22:13:38.713327Z","shell.execute_reply.started":"2023-08-26T22:13:26.327993Z"},"trusted":true},"outputs":[],"source":["number_of_samples = 8\n","fig_size_width = 12\n","sample_indices = list(range(len(path_files_images_test)))\n","\n","for i in random.sample(sample_indices, number_of_samples):\n","    \n","    # extract the sample\n","    path_file_image = path_files_images_test[i]\n","    path_file_mask = path_files_masks_test[i]\n","    path_file_labels_boxes = path_files_labels_boxes_test[i]\n","\n","    # create the needed subplots and set figure size\n","    fig, ((ax1, ax3), (ax2, ax4)) = plt.subplots(nrows=2, ncols=2)\n","    fig.set_size_inches(fig_size_width, int(fig_size_width / (INPUT_IMAGE_SHAPE[1] / INPUT_IMAGE_SHAPE[0])))    \n","\n","    # --------------------------------------------------------------------------------\n","    # read - image sample\n","    # --------------------------------------------------------------------------------\n","    # read image\n","    image = Image.open(path_file_image)\n","\n","    # add batch dimension to image\n","    image_batch = np.array(image).astype(np.float32)\n","    image_batch = np.expand_dims(image, axis=0)\n","\n","    # convert to array of integers\n","    image = np.array(image)\n","    image = image.astype(np.int32)\n","\n","    # --------------------------------------------------------------------------------\n","    # read - segmentation mask sample\n","    # --------------------------------------------------------------------------------\n","    # read mask\n","    mask = Image.open(path_file_mask)\n","\n","    # keep the 3 classes on rgb channels\n","    mask = tf.slice(tf.one_hot(mask, depth=4, dtype=tf.float32), begin=[0, 0, 1], size=[-1, -1, 3])\n","\n","    # --------------------------------------------------------------------------------\n","    # read - labels boxes sample\n","    # --------------------------------------------------------------------------------    \n","    # read ground truth labels boxes from csv file\n","    with open(path_file_labels_boxes, 'r') as f:\n","        labels_boxes = list(csv.reader(f))\n","    \n","    # --------------------------------------------------------------------------------\n","    # plot - ground truth\n","    # --------------------------------------------------------------------------------\n","    # plot the image\n","    ax1.imshow(image, vmin=0, vmax=1)\n","    ax1.set_axis_off()\n","    ax1.set_title(f'ground truth - object detection')\n","    \n","    # plot ground truth boxes\n","    for label, xmin, ymin, xmax, ymax in labels_boxes:\n","        label = int(label)\n","        xmin = float(xmin)\n","        ymin = float(ymin)\n","        xmax = float(xmax)\n","        ymax = float(ymax)        \n","        rect = patches.Rectangle((xmin, ymin), xmax - xmin + 1, ymax - ymin + 1, linewidth=1, edgecolor=LABEL_CODE_TO_COLOR[label], facecolor='none')\n","        ax1.add_patch(rect)\n","        ax1.text(xmin, ymin, LABEL_CODE_TO_DESC[label], fontsize=8, color=LABEL_CODE_TO_COLOR[label], verticalalignment='top')        \n","\n","    # plot ground truth mask\n","    ax2.imshow(mask, vmin=0, vmax=1)\n","    ax2.set_axis_off()\n","    ax2.set_title('ground truth - segmentation mask')\n","\n","    # --------------------------------------------------------------------------------\n","    # plot - model predictions\n","    # --------------------------------------------------------------------------------\n","    # get predictions from the model\n","    output_mask, output_object_detection = model_inference(image_batch, training=False)\n","    if output_object_detection.ndim > 2:\n","        output_object_detection = tf.squeeze(output_object_detection, axis=0)\n","\n","    # keep the 3 classes on rgb channels\n","    output_mask = tf.math.argmax(tf.squeeze(output_mask, axis=0), axis=-1)\n","    output_mask = tf.one_hot(output_mask, depth=4, axis=2)\n","    output_mask = tf.slice(output_mask, begin=[0, 0, 1], size=[-1, -1, 3])\n","\n","    # plot the image\n","    ax3.imshow(image, vmin=0, vmax=255)\n","    ax3.set_axis_off()\n","    ax3.set_title(f'model - object detection')\n","\n","    # plot predicted boxes\n","    for label, probability, xmin, ymin, xmax, ymax in output_object_detection:\n","        if label == LABEL_CODE_BACKGROUND:\n","            continue\n","        label = int(label)\n","        probability = int(probability * 100)\n","        xmin = float(xmin)\n","        ymin = float(ymin)\n","        xmax = float(xmax)\n","        ymax = float(ymax)        \n","        rect = patches.Rectangle((xmin, ymin), xmax - xmin + 1, ymax - ymin + 1, linewidth=1, edgecolor=LABEL_CODE_TO_COLOR[label], facecolor='none')\n","        ax3.add_patch(rect)\n","        ax3.text(xmin, ymin, f'{LABEL_CODE_TO_DESC[label]} {probability}%', fontsize=8, color=LABEL_CODE_TO_COLOR[label], verticalalignment='top')        \n","\n","    # plot predicted mask\n","    ax4.imshow(output_mask, vmin=0, vmax=1)\n","    ax4.set_axis_off()\n","    ax4.set_title('model - segmentation mask')\n","\n","    # show the plot\n","    plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
