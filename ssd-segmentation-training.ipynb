{"cells":[{"cell_type":"markdown","metadata":{},"source":["# global variables"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:41:36.062410Z","iopub.status.busy":"2023-09-09T13:41:36.062085Z","iopub.status.idle":"2023-09-09T13:41:36.076577Z","shell.execute_reply":"2023-09-09T13:41:36.075311Z","shell.execute_reply.started":"2023-09-09T13:41:36.062381Z"},"trusted":true},"outputs":[],"source":["# check environment\n","from os import environ\n","IS_KAGGLE_ENVIRONMENT = 'KAGGLE_KERNEL_RUN_TYPE' in environ\n","\n","# models path\n","MODELS_PATH = '/kaggle/working/models/experiment-01/' if IS_KAGGLE_ENVIRONMENT else 'data/models/experiment-01/'\n","MODEL_NAME = 'mobilenetv2-deeplabv3plus_3_6_12-ssdlite'\n","# MODEL_NAME = 'shufflenetv2-1x-deeplabv3plus_3_6_12-ssdlite'\n","\n","# data options\n","INPUT_IMAGE_SHAPE = (480, 640, 3)\n","LABELS_CODES = [0, 1, 2, 3]\n","LABEL_CODE_BACKGROUND = 0\n","NUMBER_OF_CLASSES = len(LABELS_CODES)\n","\n","# object detection options\n","STANDARD_DEVIATIONS_CENTROIDS_OFFSETS = (0.1, 0.1, 0.2, 0.2)\n","\n","# labels conversions\n","LABEL_CODE_TO_DESC = {\n","    1: 'monorail',\n","    2: 'person',\n","    3: 'forklift'\n","}\n","LABEL_CODE_TO_COLOR = {\n","    1: 'red',\n","    2: 'green',\n","    3: 'blue'\n","}\n","\n","# tensorflow options\n","BATCH_SIZE = 16\n","SEED = 1993"]},{"cell_type":"markdown","metadata":{},"source":["# kaggle setup"]},{"cell_type":"markdown","metadata":{},"source":["## clone repository and setup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:41:36.078856Z","iopub.status.busy":"2023-09-09T13:41:36.078482Z","iopub.status.idle":"2023-09-09T13:41:41.410300Z","shell.execute_reply":"2023-09-09T13:41:41.408944Z","shell.execute_reply.started":"2023-09-09T13:41:36.078794Z"},"trusted":true},"outputs":[],"source":["if IS_KAGGLE_ENVIRONMENT:\n","    # check if repo folder exists, eventually delete it\n","    %cd '/kaggle/working/'\n","    import os\n","    if os.path.exists('ssd-segmentation'):\n","        !rm -r 'ssd-segmentation'\n","\n","    # clone github repository\n","    !git clone 'https://github.com/matteo-stat/ssd-segmentation.git'\n","\n","    # change working directory to cloned repository folder\n","    %cd '/kaggle/working/ssd-segmentation'\n","\n","    # change branch\n","    # !git checkout 'main'\n","\n","    # show working directory content\n","    !ls"]},{"cell_type":"markdown","metadata":{},"source":["# dependecies"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:41:41.413742Z","iopub.status.busy":"2023-09-09T13:41:41.413418Z","iopub.status.idle":"2023-09-09T13:41:49.293574Z","shell.execute_reply":"2023-09-09T13:41:49.292595Z","shell.execute_reply.started":"2023-09-09T13:41:41.413712Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","tf.keras.saving.get_custom_objects().clear()\n","tf.random.set_seed(SEED)\n","\n","import random\n","random.seed(SEED)\n","\n","import json\n","import csv\n","import numpy as np\n","from matplotlib import pyplot as plt, patches\n","from PIL import Image\n","import ssdseglib"]},{"cell_type":"markdown","metadata":{},"source":["# default bounding boxes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:41:49.296109Z","iopub.status.busy":"2023-09-09T13:41:49.294793Z","iopub.status.idle":"2023-09-09T13:41:49.305319Z","shell.execute_reply":"2023-09-09T13:41:49.304358Z","shell.execute_reply.started":"2023-09-09T13:41:49.296078Z"},"trusted":true},"outputs":[],"source":["# create default bounding boxes\n","boxes_default = ssdseglib.boxes.DefaultBoundingBoxes(\n","    feature_maps_shapes=((30, 40), (15, 20), (8, 10), (4, 5)),\n","    centers_padding_from_borders_percentage=(0.025, 0.05, 0.075, 0.1),\n","    boxes_scales=(0.15, 0.95),\n","    additional_square_box=True,\n",")\n","\n","# rescale default bounding boxes to input image shape\n","boxes_default.rescale_boxes_coordinates(image_shape=INPUT_IMAGE_SHAPE[:2])"]},{"cell_type":"markdown","metadata":{},"source":["# data encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:41:49.308597Z","iopub.status.busy":"2023-09-09T13:41:49.308011Z","iopub.status.idle":"2023-09-09T13:41:52.167500Z","shell.execute_reply":"2023-09-09T13:41:52.166506Z","shell.execute_reply.started":"2023-09-09T13:41:49.308565Z"},"trusted":true},"outputs":[],"source":["# create a data reader encoder\n","data_reader_encoder = ssdseglib.datacoder.DataEncoderDecoder(\n","    num_classes=NUMBER_OF_CLASSES,\n","    image_shape=INPUT_IMAGE_SHAPE[:2],\n","    xmin_boxes_default=boxes_default.get_boxes_coordinates_xmin(coordinates_style='ssd'),\n","    ymin_boxes_default=boxes_default.get_boxes_coordinates_ymin(coordinates_style='ssd'),\n","    xmax_boxes_default=boxes_default.get_boxes_coordinates_xmax(coordinates_style='ssd'),\n","    ymax_boxes_default=boxes_default.get_boxes_coordinates_ymax(coordinates_style='ssd'),\n","    iou_threshold=0.525,\n","    standard_deviations_centroids_offsets=STANDARD_DEVIATIONS_CENTROIDS_OFFSETS,\n","    augmentation_horizontal_flip=True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# input data"]},{"cell_type":"markdown","metadata":{},"source":["## load metadata"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:41:52.169156Z","iopub.status.busy":"2023-09-09T13:41:52.168814Z","iopub.status.idle":"2023-09-09T13:41:52.195632Z","shell.execute_reply":"2023-09-09T13:41:52.194752Z","shell.execute_reply.started":"2023-09-09T13:41:52.169124Z"},"trusted":true},"outputs":[],"source":["# training\n","data = []\n","\n","# train\n","with open('data/train.json', 'r') as f:\n","    data.extend(json.load(f))\n","\n","# train additional - persons\n","with open('data/train-additional-persons.json', 'r') as f:\n","    persons = json.load(f)\n","    persons = random.sample(persons, int(len(persons)*0.8))\n","    data.extend(persons)\n","\n","# train additional - forklifts\n","with open('data/train-additional-forklifts.json', 'r') as f:\n","    data.extend(json.load(f))\n","\n","# the training set it's small and the validation set even smaller..\n","# it's so small that probably any metrics on it won't be particularly reliable \n","# at this point maybe it's just better to use the validation set as additional training data\n","with open('data/eval-persons-forklifts.json', 'r') as f:\n","    data.extend(json.load(f))\n","\n","# unpack train metadata into separate lists\n","path_files_images_train, path_files_masks_train, path_files_labels_boxes_train = map(list, zip(*data))\n","\n","# test\n","with open('data/test.json', 'r') as f:\n","    path_files_images_test, path_files_masks_test, path_files_labels_boxes_test = map(list, zip(*json.load(f)))\n","\n","# replace local data directory with kaggle input directory\n","if IS_KAGGLE_ENVIRONMENT:\n","    path_data_kaggle = '/kaggle/input/ssd-segmentation-dataset-v2/'\n","    path_files_images_train = [path.replace('data/', f'{path_data_kaggle}data/') for path in path_files_images_train]\n","    path_files_masks_train = [path.replace('data/',  f'{path_data_kaggle}data/') for path in path_files_masks_train]\n","    path_files_labels_boxes_train = [path.replace('data/',  f'{path_data_kaggle}data/') for path in path_files_labels_boxes_train]\n","\n","    path_files_images_test = [path.replace('data/',  f'{path_data_kaggle}data/') for path in path_files_images_test]\n","    path_files_masks_test = [path.replace('data/',  f'{path_data_kaggle}data/') for path in path_files_masks_test]\n","    path_files_labels_boxes_test = [path.replace('data/',  f'{path_data_kaggle}data/') for path in path_files_labels_boxes_test]"]},{"cell_type":"markdown","metadata":{},"source":["## tensorflow datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:41:52.197446Z","iopub.status.busy":"2023-09-09T13:41:52.196942Z","iopub.status.idle":"2023-09-09T13:41:54.096470Z","shell.execute_reply":"2023-09-09T13:41:54.095481Z","shell.execute_reply.started":"2023-09-09T13:41:52.197409Z"},"trusted":true},"outputs":[],"source":["# training\n","ds_train = (\n","    tf.data.Dataset.from_tensor_slices((path_files_images_train, path_files_masks_train, path_files_labels_boxes_train))\n","    .shuffle(buffer_size=len(path_files_images_train))\n","    .map(data_reader_encoder.read_and_encode, num_parallel_calls=tf.data.AUTOTUNE)\n","    .batch(batch_size=BATCH_SIZE)\n","    .map(ssdseglib.datacoder.augmentation_rgb_channels, num_parallel_calls=tf.data.AUTOTUNE)\n","    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",")\n","\n","# eval\n","ds_eval = (\n","    tf.data.Dataset.from_tensor_slices((path_files_images_test, path_files_masks_test, path_files_labels_boxes_test))\n","    .shuffle(buffer_size=len(path_files_images_test))\n","    .map(data_reader_encoder.read_and_encode, num_parallel_calls=tf.data.AUTOTUNE)\n","    .batch(batch_size=BATCH_SIZE)\n","    .map(ssdseglib.datacoder.augmentation_rgb_channels, num_parallel_calls=tf.data.AUTOTUNE)\n","    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",")\n","\n","# test\n","ds_test = (\n","    tf.data.Dataset.from_tensor_slices(path_files_images_test)\n","    .map(ssdseglib.datacoder.read_image, num_parallel_calls=tf.data.AUTOTUNE)\n","    .batch(batch_size=BATCH_SIZE)\n","    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",")\n","\n","# train nms tuning\n","ds_train_nms_tuning = (\n","    tf.data.Dataset.from_tensor_slices(path_files_images_train)\n","    .map(ssdseglib.datacoder.read_image, num_parallel_calls=tf.data.AUTOTUNE)\n","    .batch(batch_size=BATCH_SIZE)\n","    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# weighted losses for model training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:41:54.098417Z","iopub.status.busy":"2023-09-09T13:41:54.098062Z","iopub.status.idle":"2023-09-09T13:41:54.107442Z","shell.execute_reply":"2023-09-09T13:41:54.106381Z","shell.execute_reply.started":"2023-09-09T13:41:54.098382Z"},"trusted":true},"outputs":[],"source":["# weighted loss for semantic segmentation\n","cross_entropy_loss = ssdseglib.losses.cross_entropy(classes_weights=(0.05, 0.575, 0.135, 0.24))"]},{"cell_type":"markdown","metadata":{},"source":["# weighted metrics for model training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:41:54.110929Z","iopub.status.busy":"2023-09-09T13:41:54.110047Z","iopub.status.idle":"2023-09-09T13:41:54.118679Z","shell.execute_reply":"2023-09-09T13:41:54.117764Z","shell.execute_reply.started":"2023-09-09T13:41:54.110893Z"},"trusted":true},"outputs":[],"source":["# weighted metrics for semantic segmentation\n","jaccard_iou_segmentation_masks_metric = ssdseglib.metrics.jaccard_iou_segmentation_masks(classes_weights=(0.05, 0.575, 0.135, 0.24))\n","\n","# weighted metrics for boxes classification\n","categorical_accuracy_metric = ssdseglib.metrics.categorical_accuracy(classes_weights=(0., 1/3, 1/3, 1/3))\n","\n","# metrics for boxes regression\n","jaccard_iou_bounding_boxes_metric = ssdseglib.metrics.jaccard_iou_bounding_boxes(\n","    center_x_boxes_default=data_reader_encoder.center_x_boxes_default,\n","    center_y_boxes_default=data_reader_encoder.center_y_boxes_default,\n","    width_boxes_default=data_reader_encoder.width_boxes_default,\n","    height_boxes_default=data_reader_encoder.height_boxes_default,\n","    standard_deviations_centroids_offsets=STANDARD_DEVIATIONS_CENTROIDS_OFFSETS\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# model"]},{"cell_type":"markdown","metadata":{},"source":["## architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:41:54.120471Z","iopub.status.busy":"2023-09-09T13:41:54.120049Z","iopub.status.idle":"2023-09-09T13:41:54.136417Z","shell.execute_reply":"2023-09-09T13:41:54.135269Z","shell.execute_reply.started":"2023-09-09T13:41:54.120438Z"},"trusted":true},"outputs":[],"source":["# model builder\n","model_builder = ssdseglib.models.MobileNetV2SsdSegBuilder(\n","    input_image_shape=INPUT_IMAGE_SHAPE,\n","    number_of_boxes_per_point=[\n","        len(aspect_ratios) + (1 if boxes_default.additional_square_box else 0)\n","        for aspect_ratios in boxes_default.feature_maps_aspect_ratios\n","    ],\n","    number_of_classes=NUMBER_OF_CLASSES,\n","    center_x_boxes_default=boxes_default.get_boxes_coordinates_center_x(coordinates_style='ssd'),\n","    center_y_boxes_default=boxes_default.get_boxes_coordinates_center_y(coordinates_style='ssd'),\n","    width_boxes_default=boxes_default.get_boxes_coordinates_width(coordinates_style='ssd'),\n","    height_boxes_default=boxes_default.get_boxes_coordinates_height(coordinates_style='ssd'),\n","    standard_deviations_centroids_offsets=STANDARD_DEVIATIONS_CENTROIDS_OFFSETS\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:58:13.337846Z","iopub.status.busy":"2023-09-09T13:58:13.337436Z","iopub.status.idle":"2023-09-09T13:58:13.348681Z","shell.execute_reply":"2023-09-09T13:58:13.347667Z","shell.execute_reply.started":"2023-09-09T13:58:13.337805Z"},"trusted":true},"outputs":[],"source":["# model builder\n","model_builder = ssdseglib.models.ShuffleNetV2SsdSegBuilder(\n","    input_image_shape=INPUT_IMAGE_SHAPE,\n","    model_size='1.5x',\n","    use_additional_depthwise_convolution=True,\n","    use_residual_connections=True,\n","    number_of_boxes_per_point=[\n","        len(aspect_ratios) + (1 if boxes_default.additional_square_box else 0)\n","        for aspect_ratios in boxes_default.feature_maps_aspect_ratios\n","    ],\n","    number_of_classes=NUMBER_OF_CLASSES,\n","    center_x_boxes_default=boxes_default.get_boxes_coordinates_center_x(coordinates_style='ssd'),\n","    center_y_boxes_default=boxes_default.get_boxes_coordinates_center_y(coordinates_style='ssd'),\n","    width_boxes_default=boxes_default.get_boxes_coordinates_width(coordinates_style='ssd'),\n","    height_boxes_default=boxes_default.get_boxes_coordinates_height(coordinates_style='ssd'),\n","    standard_deviations_centroids_offsets=STANDARD_DEVIATIONS_CENTROIDS_OFFSETS\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:58:13.915259Z","iopub.status.busy":"2023-09-09T13:58:13.914078Z","iopub.status.idle":"2023-09-09T13:58:16.659182Z","shell.execute_reply":"2023-09-09T13:58:16.657966Z","shell.execute_reply.started":"2023-09-09T13:58:13.915187Z"},"trusted":true},"outputs":[],"source":["# model for training\n","model = model_builder.get_model_for_training(\n","    segmentation_architecture='deeplabv3plus',\n","    object_detection_architecture='ssdlite',\n","    segmentation_dilation_rates=(3, 6, 12)    \n",")\n","\n","# or maybe load a trained model and continue the training\n","# model = tf.keras.models.load_model(f'{MODELS_PATH}{MODEL_NAME}-75epoch.keras', compile=False)\n","\n","# print model summary\n","# model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:58:17.758805Z","iopub.status.busy":"2023-09-09T13:58:17.758432Z","iopub.status.idle":"2023-09-09T13:58:17.767125Z","shell.execute_reply":"2023-09-09T13:58:17.766048Z","shell.execute_reply.started":"2023-09-09T13:58:17.758776Z"},"trusted":true},"outputs":[],"source":["# optimizer\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"]},{"cell_type":"markdown","metadata":{},"source":["## compile"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:58:18.908041Z","iopub.status.busy":"2023-09-09T13:58:18.907319Z","iopub.status.idle":"2023-09-09T13:58:18.937565Z","shell.execute_reply":"2023-09-09T13:58:18.936511Z","shell.execute_reply.started":"2023-09-09T13:58:18.907998Z"},"trusted":true},"outputs":[],"source":["# each ouput has its own loss and metrics\n","model.compile(\n","    optimizer=optimizer,\n","    loss={\n","        'output-mask': cross_entropy_loss,\n","        'output-labels': ssdseglib.losses.confidence_loss,\n","        'output-boxes': ssdseglib.losses.localization_loss\n","    },\n","    loss_weights={\n","        'output-mask': 1.0,\n","        'output-labels': 1.0,\n","        'output-boxes': 1.0\n","    },\n","    metrics={\n","        'output-mask': jaccard_iou_segmentation_masks_metric,\n","        'output-labels': categorical_accuracy_metric,\n","        'output-boxes': jaccard_iou_bounding_boxes_metric,\n","    }\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## early stopping"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:58:20.012771Z","iopub.status.busy":"2023-09-09T13:58:20.012076Z","iopub.status.idle":"2023-09-09T13:58:20.018181Z","shell.execute_reply":"2023-09-09T13:58:20.017089Z","shell.execute_reply.started":"2023-09-09T13:58:20.012733Z"},"trusted":true},"outputs":[],"source":["early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss',\n","    min_delta=0.5,\n","    patience=20,\n","    verbose=1,\n","    restore_best_weights=True,\n","    start_from_epoch=0,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## training model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-09T13:58:22.207661Z","iopub.status.busy":"2023-09-09T13:58:22.207281Z"},"trusted":true},"outputs":[],"source":["# fit the model\n","history = model.fit(\n","    ds_train,\n","    epochs=40,\n","    validation_data=ds_eval,\n","    # callbacks=[early_stopping]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### training history"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# plot training loss and validation loss\n","plt.figure(figsize=(10, 6))\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Test Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')  \n","plt.ylabel('Loss')\n","plt.legend() \n","plt.grid(True)\n","plt.show()  "]},{"cell_type":"markdown","metadata":{},"source":["## save weights"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# save model\n","# model.save(f'{MODELS_PATH}{MODEL_NAME}-135-epoch.keras')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!zip -r '/kaggle/working/models/experiment-01.zip' '/kaggle/working/models/experiment-01'"]},{"cell_type":"markdown","metadata":{},"source":["## inference model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# load trained model\n","model_trained = tf.keras.models.load_model(f'{MODELS_PATH}{MODEL_NAME}-105-epoch.keras', compile=False)"]},{"cell_type":"markdown","metadata":{},"source":["### nms hypertuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# thresholds grid\n","boxes_iou_thresholds = (0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.20, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5)\n","labels_probability_thresholds = (0.6, 0.625, 0.65, 0.675, 0.7, 0.725, 0.75, 0.775)\n","\n","# set the iou thresholds to use for evaluate average precision in object detection\n","map_iou_thresholds_object_detection = [0.5, 0.6, 0.75]\n","\n","# calculate the maximum label length\n","length_longest_label = max(len(label) for label in LABEL_CODE_TO_DESC.values())\n","\n","# try every thresholds combination and find the best one\n","for boxes_iou_threshold in boxes_iou_thresholds:\n","    for labels_probability_threshold in labels_probability_thresholds:\n","\n","        print('-------------------------------------------------------------------------------------------------------')\n","        print(f'> boxes iou threshold: {boxes_iou_threshold}, labels probability threshold: {labels_probability_threshold}')\n","        print('-------------------------------------------------------------------------------------------------------')\n","\n","        # initialize the inference model\n","        model_inference = model_builder.get_model_for_inference(\n","            model_trained=model_trained,\n","            max_number_of_boxes_per_class=15,\n","            max_number_of_boxes_per_sample=20,\n","            boxes_iou_threshold=boxes_iou_threshold,\n","            labels_probability_threshold=labels_probability_threshold,\n","            suppress_background_boxes=False,\n","            use_segmentation_suppression=True\n","        )\n","\n","        # get model predictions\n","        _, detection_pred_batch = model_inference.predict(ds_train)\n","\n","        # split and format predictions as required by the evaluators\n","        labels_pred_batch = detection_pred_batch[:, :, 0].astype(np.int32)\n","        confidences_pred_batch = detection_pred_batch[:, :, 1].astype(np.float32)\n","        boxes_pred_batch = detection_pred_batch[:, :, 2:].astype(np.float32)\n","\n","        # for each iou threshold calculate AP and mAP\n","        for map_iou_threshold in map_iou_thresholds_object_detection:\n","            average_precision_per_class = ssdseglib.evaluators.average_precision_object_detection(\n","                labels_pred_batch=labels_pred_batch,\n","                confidences_pred_batch=confidences_pred_batch,\n","                boxes_pred_batch=boxes_pred_batch,\n","                iou_threshold=map_iou_threshold,\n","                path_files_labels_boxes=path_files_labels_boxes_train,\n","                labels_codes=LABELS_CODES,\n","                label_code_background=LABEL_CODE_BACKGROUND\n","            )\n","\n","            # iou threshold formatted for printing\n","            iou_threshold = format(map_iou_threshold, '.2f').lstrip('0')\n","\n","            # print\n","            print('\\n****************')\n","            print(f'***  AP@{iou_threshold}  ***')\n","            print('****************')\n","            for label, average_precision in average_precision_per_class.items():\n","                print(f'> {LABEL_CODE_TO_DESC[label]:>{length_longest_label}}: {average_precision:2.2f}')\n","            print('----------------')\n","            print(f'> {f\"mAP@{iou_threshold}\":>{length_longest_label}}: {sum(average_precision_per_class.values()) / len(average_precision_per_class):.2f}')            "]},{"cell_type":"markdown","metadata":{},"source":["### transfer weights"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# transfer weights\n","model_inference = model_builder.get_model_for_inference(\n","    model_trained=model_trained,\n","    max_number_of_boxes_per_class=10,\n","    max_number_of_boxes_per_sample=15,\n","    boxes_iou_threshold=0.15,\n","    labels_probability_threshold=0.72,\n","    suppress_background_boxes=False,\n","    use_segmentation_suppression=True\n",")\n","\n","# print model summary\n","# model_inference.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## evaluation"]},{"cell_type":"markdown","metadata":{},"source":["### evaluation dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# get predictions for the whole test set\n","segmentation_pred_batch, detection_pred_batch = model_inference.predict(ds_test)\n","\n","# split and format predictions as required by the evaluators\n","segmentation_pred_batch = segmentation_pred_batch.astype(np.float32)\n","labels_pred_batch = detection_pred_batch[:, :, 0].astype(np.int32)\n","confidences_pred_batch = detection_pred_batch[:, :, 1].astype(np.float32)\n","boxes_pred_batch = detection_pred_batch[:, :, 2:].astype(np.float32)"]},{"cell_type":"markdown","metadata":{},"source":["### jaccard iou"]},{"cell_type":"code","execution_count":57,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","****************\n","***   IoU    ***\n","****************\n","> monorail: 0.50\n",">   person: 0.18\n","> forklift: 0.30\n","----------------\n",">    mIoU@: 0.33\n"]}],"source":["# evaluate iou for each class\n","iou_per_class = ssdseglib.evaluators.jaccard_iou_semantic_segmentation(\n","    masks_pred_batch=segmentation_pred_batch,\n","    path_files_masks=path_files_masks_test,\n","    labels_codes=LABELS_CODES,\n","    label_code_background=LABEL_CODE_BACKGROUND,\n",")\n","\n","# calculate the maximum label length\n","length_longest_label = max(len(label) for label in LABEL_CODE_TO_DESC.values())\n","\n","# print\n","print('\\n****************')\n","print(f'***   IoU    ***')\n","print('****************')\n","for label, iou in iou_per_class.items():\n","    print(f'> {LABEL_CODE_TO_DESC[label]:>{length_longest_label}}: {iou:2.2f}')\n","print('----------------')\n","print(f'> {\"mIoU@\":>{length_longest_label}}: {sum(iou_per_class.values()) / len(iou_per_class):.2f}') "]},{"cell_type":"markdown","metadata":{},"source":["### average precision"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# set the iou thresholds to use for evaluate average precision in object detection\n","iou_thresholds_object_detection = [0.5, 0.6, 0.75, 0.85, 0.95]\n","\n","# calculate the maximum label length\n","length_longest_label = max(len(label) for label in LABEL_CODE_TO_DESC.values())\n","\n","# for each iou threshold calculate AP and mAP\n","for iou_threshold in iou_thresholds_object_detection:\n","    average_precision_per_class = ssdseglib.evaluators.average_precision_object_detection(\n","        labels_pred_batch=labels_pred_batch,\n","        confidences_pred_batch=confidences_pred_batch,\n","        boxes_pred_batch=boxes_pred_batch,\n","        iou_threshold=iou_threshold,\n","        path_files_labels_boxes=path_files_labels_boxes_test,\n","        labels_codes=LABELS_CODES,\n","        label_code_background=LABEL_CODE_BACKGROUND\n","    )\n","\n","    # iou threshold formatted for printing\n","    iou_threshold = format(iou_threshold, '.2f').lstrip('0')\n","\n","    # print\n","    print('\\n****************')\n","    print(f'***  AP@{iou_threshold}  ***')\n","    print('****************')\n","    for label, average_precision in average_precision_per_class.items():\n","        print(f'> {LABEL_CODE_TO_DESC[label]:>{length_longest_label}}: {average_precision:2.2f}')\n","    print('----------------')\n","    print(f'> {f\"mAP@{iou_threshold}\":>{length_longest_label}}: {sum(average_precision_per_class.values()) / len(average_precision_per_class):.2f}')    "]},{"cell_type":"markdown","metadata":{},"source":["# predict"]},{"cell_type":"markdown","metadata":{},"source":["## plot some predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["number_of_samples = 16\n","fig_size_width = 12\n","sample_indices = list(range(len(path_files_images_test)))\n","\n","for i in random.sample(sample_indices, number_of_samples):\n","    \n","    # extract the sample\n","    path_file_image = path_files_images_test[i]\n","    path_file_mask = path_files_masks_test[i]\n","    path_file_labels_boxes = path_files_labels_boxes_test[i]\n","\n","    # create the needed subplots and set figure size\n","    fig, ((ax1, ax3), (ax2, ax4)) = plt.subplots(nrows=2, ncols=2)\n","    fig.set_size_inches(fig_size_width, int(fig_size_width / (INPUT_IMAGE_SHAPE[1] / INPUT_IMAGE_SHAPE[0])))    \n","\n","    # --------------------------------------------------------------------------------\n","    # read - image sample\n","    # --------------------------------------------------------------------------------\n","    # read image\n","    image = Image.open(path_file_image)\n","\n","    # add batch dimension to image\n","    image_batch = np.array(image).astype(np.float32)\n","    image_batch = np.expand_dims(image, axis=0)\n","\n","    # convert to array of integers\n","    image = np.array(image)\n","    image = image.astype(np.int32)\n","\n","    # --------------------------------------------------------------------------------\n","    # read - segmentation mask sample\n","    # --------------------------------------------------------------------------------\n","    # read mask\n","    mask = Image.open(path_file_mask)\n","\n","    # keep the 3 classes on rgb channels\n","    mask = tf.slice(tf.one_hot(mask, depth=4, dtype=tf.float32), begin=[0, 0, 1], size=[-1, -1, 3])\n","\n","    # --------------------------------------------------------------------------------\n","    # read - labels boxes sample\n","    # --------------------------------------------------------------------------------    \n","    # read ground truth labels boxes from csv file\n","    with open(path_file_labels_boxes, 'r') as f:\n","        labels_boxes = list(csv.reader(f))\n","    \n","    # --------------------------------------------------------------------------------\n","    # plot - ground truth\n","    # --------------------------------------------------------------------------------\n","    # plot the image\n","    ax1.imshow(image, vmin=0, vmax=1)\n","    ax1.set_axis_off()\n","    ax1.set_title(f'ground truth - object detection')\n","    \n","    # plot ground truth boxes\n","    for label, xmin, ymin, xmax, ymax in labels_boxes:\n","        label = int(label)\n","        xmin = float(xmin)\n","        ymin = float(ymin)\n","        xmax = float(xmax)\n","        ymax = float(ymax)        \n","        rect = patches.Rectangle((xmin, ymin), xmax - xmin + 1, ymax - ymin + 1, linewidth=1, edgecolor=LABEL_CODE_TO_COLOR[label], facecolor='none')\n","        ax1.add_patch(rect)\n","        ax1.text(xmin, ymin, LABEL_CODE_TO_DESC[label], fontsize=8, color=LABEL_CODE_TO_COLOR[label], verticalalignment='top')        \n","\n","    # plot ground truth mask\n","    ax2.imshow(mask, vmin=0, vmax=1)\n","    ax2.set_axis_off()\n","    ax2.set_title('ground truth - segmentation mask')\n","\n","    # --------------------------------------------------------------------------------\n","    # plot - model predictions\n","    # --------------------------------------------------------------------------------\n","    # get predictions from the model\n","    output_mask, output_object_detection = model_inference(image_batch, training=False)\n","    if output_object_detection.ndim > 2:\n","        output_object_detection = tf.squeeze(output_object_detection, axis=0)\n","\n","    # keep the 3 classes on rgb channels\n","    output_mask = tf.math.argmax(tf.squeeze(output_mask, axis=0), axis=-1)\n","    output_mask = tf.one_hot(output_mask, depth=4, axis=2)\n","    output_mask = tf.slice(output_mask, begin=[0, 0, 1], size=[-1, -1, 3])\n","\n","    # plot the image\n","    ax3.imshow(image, vmin=0, vmax=255)\n","    ax3.set_axis_off()\n","    ax3.set_title(f'model - object detection')\n","\n","    # plot predicted boxes\n","    for label, probability, xmin, ymin, xmax, ymax in output_object_detection:\n","        if label == LABEL_CODE_BACKGROUND:\n","            continue\n","        label = int(label)\n","        probability = int(probability * 100)\n","        xmin = float(xmin)\n","        ymin = float(ymin)\n","        xmax = float(xmax)\n","        ymax = float(ymax)        \n","        rect = patches.Rectangle((xmin, ymin), xmax - xmin + 1, ymax - ymin + 1, linewidth=1, edgecolor=LABEL_CODE_TO_COLOR[label], facecolor='none')\n","        ax3.add_patch(rect)\n","        ax3.text(xmin, ymin, f'{LABEL_CODE_TO_DESC[label]} {probability}%', fontsize=8, color=LABEL_CODE_TO_COLOR[label], verticalalignment='top')        \n","\n","    # plot predicted mask\n","    ax4.imshow(output_mask, vmin=0, vmax=1)\n","    ax4.set_axis_off()\n","    ax4.set_title('model - segmentation mask')\n","\n","    # show the plot\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
